{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install scikit-posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1139,
     "status": "ok",
     "timestamp": 1634572848421,
     "user": {
      "displayName": "Iris Hardege",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04839688462566642023"
     },
     "user_tz": -60
    },
    "id": "_N3Mi0wKy1aG"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import stats\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfPEwp678wDW"
   },
   "source": [
    "### Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import filenames from tierpsy summary\n",
    "files = pd.read_csv(r'path to filenames.csv', skiprows=4)\n",
    "dictionary = pd.Series(files.filename.values,index=files.file_id).to_dict()\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import features from tierpsy summary\n",
    "feat = pd.read_csv(r'path to features.csv', skiprows=1)\n",
    "\n",
    "#get filenames\n",
    "feat['filename']= feat['file_id'].map(dictionary)\n",
    "\n",
    "#get strain names\n",
    "strain = []\n",
    "file = []\n",
    "for p in feat['filename']:\n",
    "    a = os.path.basename(p)\n",
    "    b = a[0:6]\n",
    "    strain.append(b)\n",
    "    file.append(a)\n",
    "feat['strain']= strain\n",
    "feat['file']= file\n",
    "\n",
    "#map a coloumn for video timepoints - can be omitted if you dont have different videos for different timepoints, make sure to remove video_timepoint from all code after this point\n",
    "def f(row):\n",
    "    if '0to5' in row['filename']:\n",
    "        val = '5'\n",
    "    elif '20to25' in row['filename']:\n",
    "        val = '25'\n",
    "    elif '40to45' in row['filename']:\n",
    "        val = '45'\n",
    "    else:\n",
    "        val = '0'\n",
    "    return val\n",
    "\n",
    "#create new column 'video_timepoint' using the function above\n",
    "feat['video_timepoint'] = feat.apply(f, axis=1)\n",
    "\n",
    "#make a unique index for each worm 'video_index'\n",
    "feat['unique_worm_index'] = feat['file'] + '_' + feat['worm_index'].astype(str)\n",
    "\n",
    "feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#how many trajectories per strain?\n",
    "feat['strain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_means = feat.groupby(['strain','video_timepoint']).mean()\n",
    "feat_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select timepoint of interest\n",
    "timepoint = '45'\n",
    "newdata = feat.loc[(feat['video_timepoint'] == timepoint)]\n",
    "\n",
    "feat_plot = feat.copy()\n",
    "\n",
    "#drop cols that are not features, stats will be done on all data from each strain, remove anything you dont want to include before this point\n",
    "feat_plot.drop(['video_timepoint', 'file_id', 'worm_index', 'n_skeletons', 'filename', 'file', 'unique_worm_index'], axis=1, inplace=True)\n",
    "\n",
    "feat_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all your features, make sure the max cols x max rows = the number of features you are planning to plot, defualt set up for 8 features\n",
    "maxc = 2 #number of cols in the grid\n",
    "maxr = 4 #number of rows in the grid\n",
    "\n",
    "h = 9 #height in inches of the graph image\n",
    "w = 16 #width in inches of the graph image\n",
    "\n",
    "a=0 #dont change these\n",
    "b=0\n",
    "\n",
    "# Initialise the subplot function using number of rows and columns\n",
    "fig, axes = plt.subplots(maxc, maxr, figsize=(w, h), sharey=False, constrained_layout = True)\n",
    "#fig.suptitle('Title')\n",
    "\n",
    "for col in feat_plot:\n",
    "    sns.barplot(ax=axes[a, b], x=feat.strain, y=feat[col])\n",
    "    axes[a,b].set_title(col)\n",
    "    axes[a,b].tick_params(labelrotation=45)\n",
    "    b = b+1\n",
    "    if b > maxr-1:\n",
    "        a = a+1\n",
    "        b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform kruskal wallis test, saved in 'kruskal_result' and dunns multiple t tests with bonferroni correction, saved in 'dunn_bonferroni_result' and displayed\n",
    "#make a lists needed for loop and stats\n",
    "strain_list = feat_plot.strain.unique()\n",
    "key=[]\n",
    "values_k=[]\n",
    "values_d=[]\n",
    "kruskal_result = {}\n",
    "dunn_bonferroni_result={}\n",
    "\n",
    "for col in feat_plot:\n",
    "    if col != 'strain':\n",
    "        group1 = newdata.query('strain == \"N2____\"')[col] #update these with your strain names, you can add more if you like by adding new groups \n",
    "        group2 = newdata.query('strain == \"lgc-41\"')[col]\n",
    "        group3 = newdata.query('strain == \"alh-11\"')[col]\n",
    "        key.append(col)\n",
    "        values_k.append(stats.kruskal(group1, group2, group3))\n",
    "        all_data = [group1, group2, group3]\n",
    "        #perform Dunn's test using a Bonferonni correction for the p-values\n",
    "        values_d.append(sp.posthoc_dunn(all_data, p_adjust = 'bonferroni'))\n",
    "        \n",
    "for i in range(len(key)):\n",
    "    kruskal_result[key[i]] = values_k[i]\n",
    "    dunn_bonferroni_result[key[i]] = values_d[i]\n",
    "    \n",
    "for key, value in dunn_bonferroni_result.items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eJf0CAbvHZV"
   },
   "outputs": [],
   "source": [
    "#if you want to export anything\n",
    "export = feat\n",
    "export.to_csv(r'export path and file name.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM2yNrWslQbW4bqjqULEUXA",
   "collapsed_sections": [],
   "name": "global_working_angularvel.ipynb",
   "provenance": [
    {
     "file_id": "135lBNaSEM5TtPFi-dIdBbwu_SfZ6UeH6",
     "timestamp": 1634565599389
    },
    {
     "file_id": "1P0Yz-7plXuqGwFPh3-hPqhEFS1LT8Dvi",
     "timestamp": 1632213237158
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
